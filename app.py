import os
import streamlit as st
from llama_index.llms.together import TogetherLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import VectorStoreIndex, Settings, StorageContext, load_index_from_storage
from huggingface_hub import snapshot_download

# API í‚¤ ê²€ì¦
if 'api_keys' not in st.secrets:
    st.error('API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .streamlit/secrets.toml íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.')
    st.stop()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì„¸ë¬´ ìƒë‹´ ì±—ë´‡",
    page_icon="ğŸ’¼",
    layout="centered"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'index' not in st.session_state:
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    # TogetherLLM ì„¤ì •
    llm = TogetherLLM(
        model="meta-llama/Llama-3.3-70B-Instruct-Turbo-Free",
        api_key=st.secrets["api_keys"]["together"],  # secretsì—ì„œ í‚¤ ê°€ì ¸ì˜¤ê¸°
        temperature=0,
        max_tokens=2048,
        system_prompt="ë„ˆëŠ” í•œêµ­ì¸ ì„¸ë¬´ì‚¬ì•¼. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ìµœëŒ€í•œ ìì„¸í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì¤˜."
    )

    # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )

    Settings.llm = llm
    Settings.embed_model = embed_model
    Settings.chunk_size = 2048
    Settings.chunk_overlap = 200

    # í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì¸ë±ìŠ¤ ë‹¤ìš´ë¡œë“œ
    repo_id = "blockenters/tax-index-dztax"
    local_dir = "tax-index-dztax"

    with st.spinner('ë°ì´í„°ë¥¼ ë¡œë”©ì¤‘ì…ë‹ˆë‹¤...'):
        # í—ˆê¹…í˜ì´ìŠ¤ì— ìˆëŠ” ë°ì´í„°ë¥¼ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œ
        snapshot_download(
            repo_id=repo_id,
            local_dir=local_dir,
            repo_type='dataset',
            token=st.secrets["api_keys"]["huggingface"]  # secretsì—ì„œ í‚¤ ê°€ì ¸ì˜¤ê¸°
        )

        # ë‹¤ìš´ë¡œë“œí•œ í´ë”ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ
        storage_context = StorageContext.from_defaults(persist_dir=local_dir)
        st.session_state.index = load_index_from_storage(storage_context)

# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

# í—¤ë”
st.header("ğŸ’¼ ì„¸ë¬´ ìƒë‹´ ì±—ë´‡")
st.markdown("---")

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì„¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ì¿¼ë¦¬ ì—”ì§„ ì„¤ì • ë° ì‘ë‹µ ìƒì„±
    query_engine = st.session_state.index.as_query_engine(
        similarity_top_k=5,
        response_mode="compact",
        streaming=True,
        temperature=0
    )

    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ í‘œì‹œ
    with st.chat_message("assistant"):
        response = query_engine.query(prompt)
        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": str(response)})

# ì‚¬ì´ë“œë°”ì— ë„ì›€ë§ ì¶”ê°€
with st.sidebar:
    st.markdown("""
    ### ğŸ’¡ ì‚¬ìš© ë°©ë²•
    1. ì„¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”
    2. ì˜ˆì‹œ ì§ˆë¬¸:
        - ì†Œë“ì„¸ ì‹ ê³  ê¸°í•œì´ ì–¸ì œì¸ê°€ìš”?
        - ë¶€ê°€ê°€ì¹˜ì„¸ ì‹ ê³ ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
        - ì—°ë§ì •ì‚° í•„ìš” ì„œë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
        - ì„¸ë¬´ì‚¬ ì‹ ê³ ëŒ€í–‰ ìˆ˜ìˆ˜ë£Œê°€ ê¶ê¸ˆí•©ë‹ˆë‹¤
    """)